### YamlMime:UniversalReference
api_name: []
items:
- children:
  - azure.synapse.spark.operations.SparkBatchOperations.cancel_spark_batch_job
  - azure.synapse.spark.operations.SparkBatchOperations.create_spark_batch_job
  - azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_job
  - azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_jobs
  - azure.synapse.spark.operations.SparkBatchOperations.models
  class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations
  inheritance:
  - type: builtins.object
  langs:
  - python
  module: azure.synapse.spark.operations
  name: SparkBatchOperations
  summary: 'SparkBatchOperations operations.


    You should not instantiate this class directly. Instead, you should create a Client
    instance that

    instantiates it for you and attaches it as an attribute.'
  syntax:
    content: SparkBatchOperations(client, config, serializer, deserializer)
    parameters:
    - description: Client for service requests.
      id: client
    - description: Configuration of service client.
      id: config
    - description: An object model serializer.
      id: serializer
    - description: An object model deserializer.
      id: deserializer
    variables:
    - description: Alias to model classes used in this operation group.
      id: models
  type: class
  uid: azure.synapse.spark.operations.SparkBatchOperations
- class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations.cancel_spark_batch_job
  langs:
  - python
  module: azure.synapse.spark.operations
  name: cancel_spark_batch_job(batch_id, **kwargs)
  namewithoutparameters: cancel_spark_batch_job
  summary: Cancels a running spark batch job.
  syntax:
    content: cancel_spark_batch_job(batch_id, **kwargs)
    parameters:
    - description: Identifier for the batch job.
      id: batch_id
      isRequired: true
      type:
      - int
    - description: A custom type or function that will be passed the direct response
      id: cls
      isRequired: true
      type:
      - callable
    return:
      description: None, or the result of cls(response)
      type:
      - None
  type: method
  uid: azure.synapse.spark.operations.SparkBatchOperations.cancel_spark_batch_job
- class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations.create_spark_batch_job
  langs:
  - python
  module: azure.synapse.spark.operations
  name: create_spark_batch_job(spark_batch_job_options, detailed=None, **kwargs)
  namewithoutparameters: create_spark_batch_job
  summary: Create new spark batch job.
  syntax:
    content: create_spark_batch_job(spark_batch_job_options, detailed=None, **kwargs)
    parameters:
    - description: Livy compatible batch job request payload.
      id: spark_batch_job_options
      isRequired: true
      type:
      - azure.synapse.spark.models.SparkBatchJobOptions
    - defaultValue: None
      description: 'Optional query param specifying whether detailed response is returned
        beyond

        plain livy.'
      id: detailed
      type:
      - bool
    - description: A custom type or function that will be passed the direct response
      id: cls
      isRequired: true
      type:
      - callable
    return:
      description: SparkBatchJob, or the result of cls(response)
      type:
      - azure.synapse.spark.models.SparkBatchJob
  type: method
  uid: azure.synapse.spark.operations.SparkBatchOperations.create_spark_batch_job
- class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_job
  langs:
  - python
  module: azure.synapse.spark.operations
  name: get_spark_batch_job(batch_id, detailed=None, **kwargs)
  namewithoutparameters: get_spark_batch_job
  summary: Gets a single spark batch job.
  syntax:
    content: get_spark_batch_job(batch_id, detailed=None, **kwargs)
    parameters:
    - description: Identifier for the batch job.
      id: batch_id
      isRequired: true
      type:
      - int
    - defaultValue: None
      description: 'Optional query param specifying whether detailed response is returned
        beyond

        plain livy.'
      id: detailed
      type:
      - bool
    - description: A custom type or function that will be passed the direct response
      id: cls
      isRequired: true
      type:
      - callable
    return:
      description: SparkBatchJob, or the result of cls(response)
      type:
      - azure.synapse.spark.models.SparkBatchJob
  type: method
  uid: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_job
- class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_jobs
  langs:
  - python
  module: azure.synapse.spark.operations
  name: get_spark_batch_jobs(from_parameter=None, size=None, detailed=None, **kwargs)
  namewithoutparameters: get_spark_batch_jobs
  summary: List all spark batch jobs which are running under a particular spark pool.
  syntax:
    content: get_spark_batch_jobs(from_parameter=None, size=None, detailed=None, **kwargs)
    parameters:
    - defaultValue: None
      description: Optional param specifying which index the list should begin from.
      id: from_parameter
      type:
      - int
    - defaultValue: None
      description: 'Optional param specifying the size of the returned list.

        By default it is 20 and that is the maximum.'
      id: size
      type:
      - int
    - defaultValue: None
      description: 'Optional query param specifying whether detailed response is returned
        beyond

        plain livy.'
      id: detailed
      type:
      - bool
    - description: A custom type or function that will be passed the direct response
      id: cls
      isRequired: true
      type:
      - callable
    return:
      description: SparkBatchJobCollection, or the result of cls(response)
      type:
      - azure.synapse.spark.models.SparkBatchJobCollection
  type: method
  uid: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_jobs
- class: azure.synapse.spark.operations.SparkBatchOperations
  fullName: azure.synapse.spark.operations.SparkBatchOperations.models
  langs:
  - python
  module: azure.synapse.spark.operations
  name: models
  syntax:
    content: models = <module 'azure.synapse.spark.models' from 'c:\\hostedtoolcache\\windows\\python\\3.6.8\\x64\\lib\\site-packages\\azure\\synapse\\spark\\models\\__init__.py'>
  type: attribute
  uid: azure.synapse.spark.operations.SparkBatchOperations.models
references:
- fullName: azure.synapse.spark.operations.SparkBatchOperations.cancel_spark_batch_job
  isExternal: false
  name: cancel_spark_batch_job(batch_id, **kwargs)
  parent: azure.synapse.spark.operations.SparkBatchOperations
  uid: azure.synapse.spark.operations.SparkBatchOperations.cancel_spark_batch_job
- fullName: azure.synapse.spark.operations.SparkBatchOperations.create_spark_batch_job
  isExternal: false
  name: create_spark_batch_job(spark_batch_job_options, detailed=None, **kwargs)
  parent: azure.synapse.spark.operations.SparkBatchOperations
  uid: azure.synapse.spark.operations.SparkBatchOperations.create_spark_batch_job
- fullName: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_job
  isExternal: false
  name: get_spark_batch_job(batch_id, detailed=None, **kwargs)
  parent: azure.synapse.spark.operations.SparkBatchOperations
  uid: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_job
- fullName: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_jobs
  isExternal: false
  name: get_spark_batch_jobs(from_parameter=None, size=None, detailed=None, **kwargs)
  parent: azure.synapse.spark.operations.SparkBatchOperations
  uid: azure.synapse.spark.operations.SparkBatchOperations.get_spark_batch_jobs
- fullName: azure.synapse.spark.operations.SparkBatchOperations.models
  isExternal: false
  name: models
  parent: azure.synapse.spark.operations.SparkBatchOperations
  uid: azure.synapse.spark.operations.SparkBatchOperations.models
