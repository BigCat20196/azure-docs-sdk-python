### YamlMime:PythonClass
uid: azure.storage.filedatalake.aio.DataLakeFileClient
name: DataLakeFileClient
fullName: azure.storage.filedatalake.aio.DataLakeFileClient
module: azure.storage.filedatalake.aio
inheritances:
- azure.storage.filedatalake.aio._path_client_async.PathClient
- azure.storage.filedatalake._data_lake_file_client.DataLakeFileClient
summary: A client to interact with the DataLake file, even if the file may not yet
  exist.
constructor:
  syntax: DataLakeFileClient(account_url, file_system_name, file_path, credential=None,
    **kwargs)
  parameters:
  - name: account_url
    description: The URI to the storage account.
    isRequired: true
    types:
    - <xref:str>
  - name: file_system_name
    description: The file system for the directory or files.
    isRequired: true
    types:
    - <xref:str>
  - name: file_path
    description: 'The whole file path, so that to interact with a specific file.

      eg. "{directory}/{subdirectory}/{file}"'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'The credentials with which to authenticate. This is optional if
      the

      account URL already has a SAS token. The value can be a SAS token string,

      an instance of a AzureSasCredential from azure.core.credentials, an account

      shared access key, or an instance of a TokenCredentials class from azure.identity.

      If the resource URI already contains a SAS token, this will be ignored in favor
      of an explicit credential

      - except in the case of AzureSasCredential, where the conflicting SAS tokens
      will raise a ValueError.'
    isRequired: true
variables:
- description: The full endpoint URL to the file system, including SAS token if used.
  name: url
  types:
  - <xref:str>
- description: The full primary endpoint URL.
  name: primary_endpoint
  types:
  - <xref:str>
- description: The hostname of the primary endpoint.
  name: primary_hostname
  types:
  - <xref:str>
examples:
- "Creating the DataLakeServiceClient from connection string.<!--[!code-python[Main](les\\\
  datalake_samples_instantiate_client_async.py )]-->\n\n<!-- literal_block {\"ids\"\
  : [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\"\
  : \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\\73\\\\azure-storage-file-datalake-12.4.0b1\\\
  \\samples\\\\datalake_samples_instantiate_client_async.py\", \"xml:space\": \"preserve\"\
  , \"force\": false, \"language\": \"python\", \"highlight_args\": {\"linenostart\"\
  : 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.storage.filedatalake.aio\
  \ import DataLakeFileClient\n   DataLakeFileClient.from_connection_string(connection_string,\
  \ \"myfilesystem\", \"mydirectory\", \"myfile\")\n\n   ````\n"
methods:
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.append_data
  name: append_data
  summary: Append data to the file.
  signature: append_data(data, offset, length=None, **kwargs)
  parameters:
  - name: data
    description: Content to be appended to file
    isRequired: true
  - name: offset
    description: start position of the data to be appended to.
    isRequired: true
  - name: length
    description: Size of the data in bytes.
    defaultValue: None
  return:
    description: dict of the response header
  examples:
  - "Append data to the file.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   await file_client.append_data(data=file_content[2048:3072], offset=2048,\
    \ length=1024)\n\n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.create_file
  name: create_file
  summary: Create a new file.
  signature: create_file(content_settings=None, metadata=None, **kwargs)
  parameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    defaultValue: None
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: metadata
    description: Name-value pairs associated with the file as metadata.
    defaultValue: None
    types:
    - <xref:dict>(<xref:str>, <xref:str>)
  return:
    description: response dict (Etag and last modified).
  examples:
  - "Create file.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client = filesystem_client.get_file_client(file_name)\n   await file_client.create_file()\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.delete_file
  name: delete_file
  summary: Marks the specified file for deletion.
  signature: delete_file(**kwargs)
  return:
    description: None
  examples:
  - "Delete file.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   await new_client.delete_file()\n\n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.download_file
  name: download_file
  summary: 'Downloads a file to the StorageStreamDownloader. The readall() method
    must

    be used to read all the content, or readinto() must be used to download the file
    into

    a stream. Using chunks() returns an async iterator which allows the user to iterate
    over the content in chunks.'
  signature: download_file(offset=None, length=None, **kwargs)
  parameters:
  - name: offset
    description: 'Start of byte range to use for downloading a section of the file.

      Must be set if length is provided.'
    defaultValue: None
    types:
    - <xref:int>
  - name: length
    description: 'Number of bytes to read from the stream. This is optional, but

      should be supplied for optimal performance.'
    defaultValue: None
    types:
    - <xref:int>
  return:
    description: A streaming object (StorageStreamDownloader)
    types:
    - <xref:azure.storage.filedatalake.aio.StorageStreamDownloader>
  examples:
  - "Return the downloaded data.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   download = await file_client.download_file()\n   downloaded_bytes = await\
    \ download.readall()\n\n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.exists
  name: exists
  summary: Returns True if a file exists and returns False otherwise.
  signature: exists(**kwargs)
  parameters:
  - name: timeout
    description: The timeout parameter is expressed in seconds.
    types:
    - <xref:int>
  return:
    description: boolean
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.flush_data
  name: flush_data
  summary: Commit the previous appended data.
  signature: flush_data(offset, retain_uncommitted_data=False, **kwargs)
  parameters:
  - name: offset
    description: 'offset is equal to the length of the file after commit the

      previous appended data.'
    isRequired: true
  - name: retain_uncommitted_data
    description: 'Valid only for flush operations.  If

      "true", uncommitted data is retained after the flush operation

      completes; otherwise, the uncommitted data is deleted after the flush

      operation.  The default is false.  Data at offsets less than the

      specified position are written to the file when flush succeeds, but

      this optional parameter allows data after the flush position to be

      retained for a future flush operation.'
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: response header in dict
  examples:
  - "Commit the previous appended data.<!--[!code-python[Main](les\\datalake_samples_file_system_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_file_system_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client = file_system_client.get_file_client(\"myfile\")\n   await file_client.create_file()\n\
    \   with open(SOURCE_FILE, \"rb\") as data:\n       length = data.tell()\n   \
    \    await file_client.append_data(data, 0)\n       await file_client.flush_data(length)\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.get_file_properties
  name: get_file_properties
  summary: 'Returns all user-defined metadata, standard HTTP properties, and

    system properties for the file. It does not return the content of the file.'
  signature: get_file_properties(**kwargs)
  return:
    types:
    - <xref:azure.storage.filedatalake.FileProperties>
  examples:
  - "Getting the properties for a file.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   properties = await file_client.get_file_properties()\n\n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.rename_file
  name: rename_file
  summary: Rename the source file.
  signature: rename_file(new_name, **kwargs)
  parameters:
  - name: new_name
    description: 'the new file name the user want to rename to.

      The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".'
    isRequired: true
    types:
    - <xref:str>
  return:
    description: the renamed file client
    types:
    - <xref:azure.storage.filedatalake.aio.DataLakeFileClient>
  examples:
  - "Rename the source file.<!--[!code-python[Main](les\\datalake_samples_upload_download_async.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download_async.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   new_client = await file_client.rename_file(file_client.file_system_name +\
    \ '/' + 'newname')\n\n   ````\n"
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.set_file_expiry
  name: set_file_expiry
  summary: Sets the time a file will expire and be deleted.
  signature: set_file_expiry(expiry_options, expires_on=None, **kwargs)
  parameters:
  - name: expiry_options
    description: 'Required. Indicates mode of the expiry time.

      Possible values include: ''NeverExpire'', ''RelativeToCreation'', ''RelativeToNow'',
      ''Absolute'''
    isRequired: true
    types:
    - <xref:str>
  - name: or int expires_on
    description: 'The time to set the file to expiry.

      When expiry_options is RelativeTo*, expires_on should be an int in milliseconds'
    defaultValue: None
    types:
    - <xref:datetime>
  - name: timeout
    description: The timeout parameter is expressed in seconds.
    types:
    - <xref:int>
  return:
    types:
    - <xref:None>
- uid: azure.storage.filedatalake.aio.DataLakeFileClient.upload_data
  name: upload_data
  summary: Upload data to a file.
  signature: upload_data(data, length=None, overwrite=False, **kwargs)
  parameters:
  - name: data
    description: Content to be uploaded to file
    isRequired: true
  - name: length
    description: Size of the data in bytes.
    defaultValue: None
    types:
    - <xref:int>
  - name: overwrite
    description: to overwrite an existing file or not.
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: response dict (Etag and last modified).
