### YamlMime:PythonClass
uid: azure.storage.filedatalake.DataLakeFileClient
name: DataLakeFileClient
fullName: azure.storage.filedatalake.DataLakeFileClient
module: azure.storage.filedatalake
inheritances:
- azure.storage.filedatalake._path_client.PathClient
summary: A client to interact with the DataLake file, even if the file may not yet
  exist.
constructor:
  syntax: DataLakeFileClient(account_url, file_system_name, file_path, credential=None,
    **kwargs)
  parameters:
  - name: account_url
    description: The URI to the storage account.
    isRequired: true
    types:
    - <xref:str>
  - name: file_system_name
    description: The file system for the directory or files.
    isRequired: true
    types:
    - <xref:str>
  - name: file_path
    description: 'The whole file path, so that to interact with a specific file.

      eg. "{directory}/{subdirectory}/{file}"'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'The credentials with which to authenticate. This is optional if
      the

      account URL already has a SAS token. The value can be a SAS token string,

      an instance of a AzureSasCredential from azure.core.credentials, an account

      shared access key, or an instance of a TokenCredentials class from azure.identity.

      If the resource URI already contains a SAS token, this will be ignored in favor
      of an explicit credential

      - except in the case of AzureSasCredential, where the conflicting SAS tokens
      will raise a ValueError.'
    isRequired: true
variables:
- description: The full endpoint URL to the file system, including SAS token if used.
  name: url
  types:
  - <xref:str>
- description: The full primary endpoint URL.
  name: primary_endpoint
  types:
  - <xref:str>
- description: The hostname of the primary endpoint.
  name: primary_hostname
  types:
  - <xref:str>
examples:
- "Creating the DataLakeServiceClient from connection string.<!--[!code-python[Main](les\\\
  datalake_samples_instantiate_client.py )]-->\n\n<!-- literal_block {\"ids\": [],\
  \ \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\"\
  : \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\\73\\\\azure-storage-file-datalake-12.4.0b1\\\
  \\samples\\\\datalake_samples_instantiate_client.py\", \"xml:space\": \"preserve\"\
  , \"force\": false, \"language\": \"python\", \"highlight_args\": {\"linenostart\"\
  : 1}, \"linenos\": false} -->\n\n````python\n\n   from azure.storage.filedatalake\
  \ import DataLakeFileClient\n   DataLakeFileClient.from_connection_string(connection_string,\
  \ \"myfilesystem\", \"mydirectory\", \"myfile\")\n\n   ````\n"
methods:
- uid: azure.storage.filedatalake.DataLakeFileClient.append_data
  name: append_data
  summary: Append data to the file.
  signature: append_data(data, offset, length=None, **kwargs)
  parameters:
  - name: data
    description: Content to be appended to file
    isRequired: true
  - name: offset
    description: start position of the data to be appended to.
    isRequired: true
  - name: length
    description: Size of the data in bytes.
    defaultValue: None
  return:
    description: dict of the response header
  examples:
  - "Append data to the file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client.append_data(data=file_content[2048:3072], offset=2048, length=1024)\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.create_file
  name: create_file
  summary: Create a new file.
  signature: create_file(content_settings=None, metadata=None, **kwargs)
  parameters:
  - name: content_settings
    description: ContentSettings object used to set path properties.
    defaultValue: None
    types:
    - <xref:azure.storage.filedatalake.ContentSettings>
  - name: metadata
    description: Name-value pairs associated with the file as metadata.
    defaultValue: None
    types:
    - <xref:dict>(<xref:str>, <xref:str>)
  return:
    description: response dict (Etag and last modified).
  examples:
  - "Create file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   file_client = filesystem_client.get_file_client(file_name)\n   file_client.create_file()\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.delete_file
  name: delete_file
  summary: Marks the specified file for deletion.
  signature: delete_file(**kwargs)
  return:
    description: None
  examples:
  - "Delete file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   new_client.delete_file()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.download_file
  name: download_file
  summary: 'Downloads a file to the StorageStreamDownloader. The readall() method
    must

    be used to read all the content, or readinto() must be used to download the file
    into

    a stream. Using chunks() returns an iterator which allows the user to iterate
    over the content in chunks.'
  signature: download_file(offset=None, length=None, **kwargs)
  parameters:
  - name: offset
    description: 'Start of byte range to use for downloading a section of the file.

      Must be set if length is provided.'
    defaultValue: None
    types:
    - <xref:int>
  - name: length
    description: 'Number of bytes to read from the stream. This is optional, but

      should be supplied for optimal performance.'
    defaultValue: None
    types:
    - <xref:int>
  return:
    description: A streaming object (StorageStreamDownloader)
    types:
    - <xref:azure.storage.filedatalake.StorageStreamDownloader>
  examples:
  - "Return the downloaded data.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   download = file_client.download_file()\n   downloaded_bytes = download.readall()\n\
    \n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.exists
  name: exists
  summary: Returns True if a file exists and returns False otherwise.
  signature: exists(**kwargs)
  return:
    description: boolean
- uid: azure.storage.filedatalake.DataLakeFileClient.flush_data
  name: flush_data
  summary: Commit the previous appended data.
  signature: flush_data(offset, retain_uncommitted_data=False, **kwargs)
  parameters:
  - name: offset
    description: 'offset is equal to the length of the file after commit the

      previous appended data.'
    isRequired: true
  - name: retain_uncommitted_data
    description: 'Valid only for flush operations.  If

      "true", uncommitted data is retained after the flush operation

      completes; otherwise, the uncommitted data is deleted after the flush

      operation.  The default is false.  Data at offsets less than the

      specified position are written to the file when flush succeeds, but

      this optional parameter allows data after the flush position to be

      retained for a future flush operation.'
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: response header in dict
  examples:
  - "Commit the previous appended data.<!--[!code-python[Main](les\\datalake_samples_file_system.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_file_system.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   with open(SOURCE_FILE, \"rb\") as data:\n       file_client = file_system_client.get_file_client(\"\
    myfile\")\n       file_client.create_file()\n       file_client.append_data(data,\
    \ 0)\n       file_client.flush_data(data.tell())\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.from_connection_string
  name: from_connection_string
  summary: 'Create DataLakeFileClient from a Connection String.


    :return a DataLakeFileClient

    :rtype ~azure.storage.filedatalake.DataLakeFileClient'
  signature: from_connection_string(conn_str, file_system_name, file_path, credential=None,
    **kwargs)
  parameters:
  - name: conn_str
    description: A connection string to an Azure Storage account.
    isRequired: true
    types:
    - <xref:str>
  - name: file_system_name
    description: The name of file system to interact with.
    isRequired: true
    types:
    - <xref:str>
  - name: directory_name
    description: The name of directory to interact with. The directory is under file
      system.
    isRequired: true
    types:
    - <xref:str>
  - name: file_name
    description: The name of file to interact with. The file is under directory.
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'The credentials with which to authenticate. This is optional if
      the

      account URL already has a SAS token, or the connection string already has shared

      access key values. The value can be a SAS token string,

      an instance of a AzureSasCredential from azure.core.credentials, an account
      shared access

      key, or an instance of a TokenCredentials class from azure.identity.

      Credentials provided here will take precedence over those in the connection
      string.'
    defaultValue: None
- uid: azure.storage.filedatalake.DataLakeFileClient.get_file_properties
  name: get_file_properties
  summary: 'Returns all user-defined metadata, standard HTTP properties, and

    system properties for the file. It does not return the content of the file.'
  signature: get_file_properties(**kwargs)
  return:
    types:
    - <xref:azure.storage.filedatalake.FileProperties>
  examples:
  - "Getting the properties for a file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   properties = file_client.get_file_properties()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.query_file
  name: query_file
  summary: 'Enables users to select/project on datalake file data by providing simple
    query expressions.

    This operations returns a DataLakeFileQueryReader, users need to use readall()
    or readinto() to get query data.'
  signature: query_file(query_expression, **kwargs)
  parameters:
  - name: query_expression
    description: 'Required. a query statement.

      eg. Select * from DataLakeStorage'
    isRequired: true
    types:
    - <xref:str>
  return:
    description: A streaming object (DataLakeFileQueryReader)
    types:
    - <xref:azure.storage.filedatalake.DataLakeFileQueryReader>
  examples:
  - "select/project on datalake file data by providing simple query expressions.<!--[!code-python[Main](les\\\
    datalake_samples_query.py )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\"\
    : [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\\
    a\\\\1\\\\s\\\\dist_temp\\\\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\
    \\datalake_samples_query.py\", \"xml:space\": \"preserve\", \"force\": false,\
    \ \"language\": \"python\", \"highlight_args\": {\"linenostart\": 1}, \"linenos\"\
    : false} -->\n\n````python\n\n   errors = []\n   def on_error(error):\n      \
    \ errors.append(error)\n\n   # upload the csv file\n   file_client = datalake_service_client.get_file_client(filesystem_name,\
    \ \"csvfile\")\n   file_client.upload_data(CSV_DATA, overwrite=True)\n\n   # select\
    \ the second column of the csv file\n   query_expression = \"SELECT _2 from DataLakeStorage\"\
    \n   input_format = DelimitedTextDialect(delimiter=',', quotechar='\"', lineterminator='\\\
    n', escapechar=\"\", has_header=False)\n   output_format = DelimitedJsonDialect(delimiter='\\\
    n')\n   reader = file_client.query_file(query_expression, on_error=on_error, file_format=input_format,\
    \ output_format=output_format)\n   content = reader.readall()\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.rename_file
  name: rename_file
  summary: Rename the source file.
  signature: rename_file(new_name, **kwargs)
  parameters:
  - name: new_name
    description: 'the new file name the user want to rename to.

      The value must have the following format: "{filesystem}/{directory}/{subdirectory}/{file}".'
    isRequired: true
    types:
    - <xref:str>
  return:
    description: the renamed file client
    types:
    - <xref:azure.storage.filedatalake.DataLakeFileClient>
  examples:
  - "Rename the source file.<!--[!code-python[Main](les\\datalake_samples_upload_download.py\
    \ )]-->\n\n<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"\
    dupnames\": [], \"backrefs\": [], \"source\": \"D:\\\\a\\\\1\\\\s\\\\dist_temp\\\
    \\73\\\\azure-storage-file-datalake-12.4.0b1\\\\samples\\\\datalake_samples_upload_download.py\"\
    , \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\", \"\
    highlight_args\": {\"linenostart\": 1}, \"linenos\": false} -->\n\n````python\n\
    \n   new_client = file_client.rename_file(file_client.file_system_name + '/' +\
    \ 'newname')\n\n   ````\n"
- uid: azure.storage.filedatalake.DataLakeFileClient.set_file_expiry
  name: set_file_expiry
  summary: Sets the time a file will expire and be deleted.
  signature: set_file_expiry(expiry_options, expires_on=None, **kwargs)
  parameters:
  - name: expiry_options
    description: 'Required. Indicates mode of the expiry time.

      Possible values include: ''NeverExpire'', ''RelativeToCreation'', ''RelativeToNow'',
      ''Absolute'''
    isRequired: true
    types:
    - <xref:str>
  - name: or int expires_on
    description: 'The time to set the file to expiry.

      When expiry_options is RelativeTo*, expires_on should be an int in milliseconds.

      If the type of expires_on is datetime, it should be in UTC time.'
    defaultValue: None
    types:
    - <xref:datetime>
  return:
    types:
    - <xref:None>
- uid: azure.storage.filedatalake.DataLakeFileClient.upload_data
  name: upload_data
  summary: Upload data to a file.
  signature: upload_data(data, length=None, overwrite=False, **kwargs)
  parameters:
  - name: data
    description: Content to be uploaded to file
    isRequired: true
  - name: length
    description: Size of the data in bytes.
    defaultValue: None
    types:
    - <xref:int>
  - name: overwrite
    description: to overwrite an existing file or not.
    defaultValue: 'False'
    types:
    - <xref:bool>
  return:
    description: response dict (Etag and last modified).
